# =============================================================================
# DevSavvy Environment Configuration
# =============================================================================
# Copy this file to .env and customize the values for your setup

# -----------------------------------------------------------------------------
# OLLAMA / LLM BACKEND CONFIGURATION
# -----------------------------------------------------------------------------

# Base URL for Ollama API (default: http://localhost:11434)
# For local Ollama: http://localhost:11434
# For remote/ngrok: https://your-ngrok-url.ngrok-free.app
VITE_OLLAMA_BASE_URL=http://localhost:11434

# Default model to use (will auto-detect if not specified)
# Common options: llama3.2, llama3, mistral, codellama, gemma:7b
VITE_DEFAULT_MODEL=llama3.2

# AI request timeout in milliseconds (default: 60000 = 1 minute)
VITE_AI_TIMEOUT=60000

# -----------------------------------------------------------------------------
# OPTIONAL: EXTERNAL SERVICES
# -----------------------------------------------------------------------------

# YouTube Data API key (for fetching video metadata)
# Get one at: https://console.cloud.google.com/apis/credentials
# VITE_YOUTUBE_API_KEY=your_youtube_api_key_here

# Backend API URL (if using a custom backend for proxying/auth)
# VITE_BACKEND_URL=http://localhost:3001

# -----------------------------------------------------------------------------
# DEVELOPMENT OPTIONS
# -----------------------------------------------------------------------------

# Enable verbose logging (true/false)
VITE_DEBUG=false

# =============================================================================
# QUICK SETUP
# =============================================================================
# 1. Install Ollama: https://ollama.ai
# 2. Pull a model: ollama pull llama3.2
# 3. Start Ollama: ollama serve
# 4. Copy this file to .env
# 5. Run: npm run dev
# =============================================================================
